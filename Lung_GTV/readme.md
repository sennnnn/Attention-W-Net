# CSTRO Lung_GTV勾画赛道总结

## 开始之前

开始之前，我应该先知道肿瘤的生长是十分随机的，而其实语义分割又是一个对像素点进行分类的问题，那么这就导致了样本特别不均衡的情况出现，这种情况下数据增强肯定是必要的，然后也应该保证一定的正负样本的比例。

## GTV训练记录

本来很天真地以为，只要按照肺部结果再做一次就好，实际上应该是有很大的不同，GTV在病人身体中生长比较随机，所以我再直接套用OAR的方法实在是太天真了，本来以为只要想分类问题那样保持正负样本的一定比例就好，实际上这样还远远不够。

## 一阶段训练情况

1~27 epoch 直接裸训练，没有加入数据增强，但是有裁剪操作的预处理操作
28~46 epoch 加入了5°的旋转数据增强
47~76 epoch 之后加入10°数据增强，但是对于结果来说没有进行丝毫的改善

## 一阶段总结

网络结构方面的问题很难说，因为就算网络结果不如其他的网络结构好，那么理论上来说，至少我的结果输出出来也能达到一定的dice值，但是输出结果之后发现，输出依然是无脑负，所以问题就在于数据预处理方面、还有损失函数上面，然后还有一点需要注意的是，在验证集上的效果还不错，这是过拟合的结果，所以接下来的尝试是，在网络中加入dropout层，更改损失函数，加入更多的防止数据不均衡的数据预处理手段。

## 二阶段训练情况

首先数据不均衡我就不能保持原来训练集的样本概率分布，正样本和负样本比例大概为：1：10，这样的话判定为负的概率太高了，然后我弄成强行让正负样本比例为：1：2左右，这样虽然能够在验证集上得到比较好的结果，因为验证集取自训练集，其正负样本比例也应该是1：2左右，但是测试集却很难，因为测试集的正负样本概率分布和真实的原始的训练集样本接近也就是大概1:10左右，所以让训练集保持一定正负样本比，这其实是不太可行的。
数据不均衡的问题真的要自闭了，我是真的佛了，昨天晚上，加了防止过拟合的措施，加了大量数据增强的措施，  
数据增强具体项目是：
1. 随机翻转  
2. 随机旋转15°左右  
经过测试之后效果也是十分一般，损失函数又不敢随便换，换了又导致一大堆坏梯度，出现损失函数nan的情况，👋。
唉，之前的损失函数实现方式是：
``` python
def weight_loss(label,logit,weight):
    cross_entropy = tf.reduce_sum(weight*label*tf.log(logit+1e-10),axis=-1)
    return cross_entropy
```
我是真的不知道为什么我这样加权定义就会梯度爆炸，我也是佛了，这篇论文是睿智论文吗？？？？  
Iqbal S, Ghani M U, Saba T, et al. Brain tumor segmentation in multi‐spectral MRI using convolutional neural networks (CNN)[J]. Microscopy research and technique, 2018, 81(4): 419-427.

## 三阶段训练

镇定思痛，已经没有多少时间了，首先正负样本比应该尽量接近真实情况，那么就不应该改变训练集原始的正负样本比，大概是1:10左右，那么就只能从损失函数入手，输出结果的时候也许argmax并不是一个好的选择，可以认为softmax之后，只要像素为肿瘤的概率达到一定的阈值，而不一定要超过与之相对的背景的概率也许也可以视为被分类为肿瘤。
由于之前的预处理函数其实有误，然后依然采用了数据集采样，正负样本强行均衡的方法来进行训练，再使用原始方法，即使用普通交叉熵函数，引入数据增强：
1. 随机平移
2. 随机翻转
3. 随机旋转
但是argmax的改变依然是有意义的

### 查错

在做数据增强的时候，对图像进行旋转的时候，数据和标签之间的旋转角度不一样，所以导致收敛很难，所以抓紧改进，马上开始训练。

## TO DO

虽然，我指定了第三阶段的训练计划，但是第三阶段有可能还是不行，所以下一步：
1. 使用加入了SE块的减少了下采样次数的自定义Unet-SE
2. 将分类问题变成异常检测的单分类问题

## cha'cuo